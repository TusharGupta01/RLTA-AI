{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning is different from supervised learning, the kind of learning studied\n",
    "in most current research in the field of machine learning. Supervised learning is learning\n",
    "from a training set of labeled examples provided by a knowledgable external supervisor.\n",
    "Each example is a description of a situation together with a specification—the label—of\n",
    "the correct action the system should take to that situation, which is often to identify a\n",
    "category to which the situation belongs. The object of this kind of learning is for the\n",
    "system to extrapolate, or generalize, its responses so that it acts correctly in situations\n",
    "not present in the training set. This is an important kind of learning, but alone it is not\n",
    "adequate for learning from interaction. In interactive problems it is often impractical to\n",
    "obtain examples of desired behavior that are both correct and representative of all the\n",
    "situations in which the agent has to act. In uncharted territory—where one would expect\n",
    "learning to be most beneficial—an agent must be able to learn from its own experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before getting into Reinforcement Learning, lets see how we work on a Supervised Learning problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/rl-4.png\" height=\"100\" width=\"500\"/><p style=\"text-align:center\">Fig-1: Example of Convolutional Neural Network Classification</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Supervised Learning, we have input variables and the target variable which we want to predict. In our case(Fig-1) we have image of a Tiger and class as the category of the animals. Here our input is pixels of the image and output is label and the network is conditional probability distribution, where condition is the pixel of the image and label is the distribution. \n",
    "\n",
    "Lets denote our input pixels as the observation(o) and target as the action(a), that we will take after looking at obervation(in our case the category of the animal). The model is the policy or a way to parameterize the probability distribution. We will denote our policy as $\\pi_{\\theta}(a|o)$, where $\\theta$ represents the parameters of the policy and $\\pi$ as the distribution over condition (o) and distribution as a."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conver this Supervised problem to sequential decision making problem, we need to add something sequential. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/rl-5.png\" height=\"100\" width=\"500\"/><p style=\"text-align:center\">Fig-2: Adding time t to make the problem sequential</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's formulate above problem with some valid action, may be you want to run if you see a tiger. Running in this case will be your action and observation is tiger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/rl-6.png\" height=\"100\" width=\"500\"/><p style=\"text-align:center\">Fig-3: Changed Action to make problem more concrete</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Fig-3 we changed the action but the target/action is still descrete values and we have maintain the decore of our previous supervised learning problem. Now suppose instead of having decrete action we want something continuous like in which direction a person should run after observing a tiger and How fast he should run? \n",
    "\n",
    "To answer above question we have to change our policy $\\pi$ such that it can take in account a distribution that can generate continuous random variable. (Fig-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/rl-7.png\" height=\"100\" width=\"500\"/><p style=\"text-align:center\">Fig-3: Actions are continuous now.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/rl-8.png\" height=\"100\" width=\"500\"/><p style=\"text-align:center\">Fig-4: Reinforcement Terminology.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$o_{t}$ is observation at any point of time.<br>\n",
    "$a_{t}$ action taken at time t <br>\n",
    "$\\pi_{\\theta}(a_{t}|o_{t})$ Policy to take an action at time t with observation at time t <br>\n",
    "$s_{t}$ state at time t. A state is what the environment looks like at the moment.<br>\n",
    "\n",
    "When have policy $\\pi_{\\theta}(a_{t}|o_{t})$ which is based on partial observation and policy $\\pi_{\\theta}(a_{t}|s_{t})$ which is based on fully observed state. Let's take and example and descibe state in a more formal way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/rl-9.png\" height=\"100\" width=\"500\"/><p style=\"text-align:center\">Fig-5: A state where a cheeta is chasing gazelle</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Fig-5 the pixels of the image is the observation and all the pixels of the image is forming a state. In other term we can say that observation are consequences of a state it can be a lossy consequence as discussed in Fig-6 where as state is the sufficient summary of the world to predict future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/rl-10.png\" height=\"100\" width=\"500\"/><p style=\"text-align:center\">Fig-6: Changing observation as a car runs infront of cheeta</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state remains same for Fig-6 but the observed pixel got changed. The state has not changed because the cheeta is still "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
